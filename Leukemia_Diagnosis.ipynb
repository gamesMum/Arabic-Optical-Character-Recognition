{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Leukemia_Diagnosis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamesMum/Arabic-Optical-Character-Recognition/blob/master/Leukemia_Diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8kkqvxDror6",
        "colab_type": "text"
      },
      "source": [
        "# Leukemia Diagnostic Model\n",
        "\n",
        "**Classification of Acute Leukemia using Pretrained Deep Convolutional Neural Networks**\n",
        "Based on the implementation in the paper:\n",
        "\n",
        "[**Human-level recognition of blast cells in acute myeloid\n",
        "leukemia with convolutional neural networks**](https://www.biorxiv.org/content/10.1101/564039v1.full.pdf)\n",
        "\n",
        " **The Dataset used in this implementation:**\n",
        "\n",
        "\n",
        "- AML dataset\n",
        "-The number of classes are 10:\n",
        "Abbreviations of morphological classes used in folder structure and annotation file\n",
        "BAS Basophil\n",
        "EBO Erythroblast (and Monoblast)\n",
        "EOS Eosinophil\n",
        "KSC Smudge cell\n",
        "LYT Lymphocyte (typical and atypical)\n",
        "MON Monocyte\n",
        "MYO Myeloblast (Metamyelocyte and Myelocyte)\n",
        "NGB Neutrophil (band)\n",
        "NGS Neutrophil (segmented)\n",
        "PMO Promyelocyte (bilobled and not)\n",
        "\n",
        "UNC Image that could not be assigned a class during re-annotation\n",
        "nan no re-annotation\n",
        "\n",
        "- link to the dataset https://www.kaggle.com/lsaa2014/single-cell-morphological-dataset-of-leukocytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycUeH3N_ror9",
        "colab_type": "text"
      },
      "source": [
        "# **Materials and Methods**\n",
        "\n",
        "Please read the paper for more and detailed information.\n",
        "\n",
        "- peripheral blood smears were selected from 100 patients diagnosed with different subtypes\n",
        "of AML at the Laboratory of Leukemia Diagnostics at Munich University Hospital between 2014 and 2017, and smears from 100 patients found to exhibit no morphological\n",
        "features of hematological malignancies in the same time frame.\n",
        "\n",
        "- The resulting digitised data consisted of multiresolution pyramidal images of a size of approximately 1 GB per scanned area of interest.\n",
        "A trained examiner experienced in routine cytomorphological diagnostics at Munich University Hospital differentiated physiological and pathological leukocyte types contained\n",
        "in the microscopic scans into the classification scheme (see fig 2B),\n",
        "which is derived from standard morphological categories and was refined to take into account subcategories relevant for the morphological classification of AML, such as bilobed Promyelocytes, which are typical of the FAB subtype M3v.\n",
        "-  Annotation was carried out on a\n",
        "single-cell basis, and approximately 100 cells were differentiated in each smear\n",
        "- Subimage patches of size 400 x 400 pixels (corresponding to approximately 29µm x 29µm)\n",
        "around the annotated cells were extracted without further cropping or filtering, including\n",
        "background components such as erythrocytes, platelets or cell fragments.\n",
        "- When examining the screened blood smears, the cytologist followed the routine clinical procedure.\n",
        "Overall, 18,365 single-cell images were annotated and cut out of the scan regions.\n",
        "\n",
        "- Annotations of single-cell images provide the ground truth for training and evaluation\n",
        "of our network.\n",
        "\n",
        "- Morphological classes containing fewer than 10 images were merged with\n",
        "neighbouring classes of the taxonomy.\n",
        "\n",
        "- A subset of 1,905 single-cell images from all morphological categories were presented to a second, independent examiner, and annotated\n",
        "for a second time in order to estimate inter-rater variability\n",
        "\n",
        "**For Implementation:**\n",
        "-\tThe network was adopted  to input image dimensions of 400 x 400 x 3\n",
        "-\tNo further cropping or ﬁltering.\n",
        "-\tRetained the cardinality hyper-parameter at C = 32.\n",
        "-\tThe ﬁnal dense layer adapted to our 10-category classiﬁcation scheme.\n",
        "-\tAnnotations of single-cell images provide the ground truth for training and evaluation of the network\n",
        "-\tThere are 10 classes for training and evaluation.\n",
        "-\tFor our image classiﬁcation task, we used a Alexnet\n",
        "-\tThe network was trained for at least 20 epochs, which took a computing time of approximately 4 days on a Nvidia GeForce GTX TITAN X GPU.\n",
        "-\tThe test group contains 20%, and the validation group 20% of the images\n",
        "-\tRandom rotational transformations of 0−359 degrees, as well as random horizontal and vertical ﬂips to the single-cell images in the dataset.\n",
        "-\tIn the end the data set was augmented in such a way that each class contained approximately 10,000 images for training. (This is from the origional paper that had 15 classes, but this number increased in this implementation since some of the classes were combined)\n",
        "- You'll see that we implemented a technique to choose and cycle between different learning rate values. This implementation is based on the paper by Leslie Smith here: https://arxiv.org/abs/1506.01186 and with the help of this great article by Thomas Dehaene\n",
        "https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee. More details found as you go along\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "y8Ds_ZtBror_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inporting the necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets, models\n",
        "import math\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision as tv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_1_2PH12rosE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print(\"CUDA is available. Training on GPU!\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Training on CPU.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BFm_X4xjrosJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#time to prepare the data\n",
        "\n",
        "batch_size = 32\n",
        "test_size = 0.20\n",
        "valid_size = 0.20\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "#define the transforms\n",
        "train_transform  = transforms.Compose([\n",
        "                                        transforms.Resize((400,400)),\n",
        "                                        transforms.RandomRotation(359),\n",
        "                                        transforms.RandomHorizontalFlip(0.2),\n",
        "                                        transforms.RandomVerticalFlip(0.2),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "\n",
        "\n",
        "train_data = datasets.ImageFolder(\"/kaggle/input/single-cell-morphological-dataset-of-leukocytes/blood_smear_images_for_aml_diagnosis_MOD/AML-Cytomorphology_LMU_MOD\",\n",
        "                                  transform = train_transform)\n",
        "\n",
        "#obtain training indicies that will be used as testing and validation\n",
        "\n",
        "num_train = len(train_data)\n",
        "indicies = list(range(num_train))\n",
        "np.random.shuffle(indicies)\n",
        "test_split = int(np.floor(test_size * num_train))\n",
        "valid_split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "train_idx, valid_idx, test_idx = indicies[test_split+valid_split:], indicies[:valid_split], indicies[valid_split:test_split+valid_split]\n",
        "\n",
        "#define samplers for obtainig the trainig, testing and validation set\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
        "                                           sampler = train_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
        "                                          sampler = test_sampler)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,\n",
        "                                          sampler = valid_sampler)\n",
        "\n",
        "\n",
        "#  PROMYELOCYTE (PMB Promyelocyte (bilobled))\n",
        "# PMO Promyelocyte), MYELOCYTE (MYB Myelocyte, MYO Myeloblast)ARE FOUND ON LEUKEMIA PATIENTS \n",
        "classes = ['BAS', 'EBO', 'EOS', 'KSC','LYT','MON', 'MYO', 'NGB', 'NGS', 'PMO']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vnCxH-B9rosN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def imshow(img):\n",
        "  img = img /2+0.5 #unormalize the images\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0))) #convert it back from tensor to image\n",
        "\n",
        "#get one batch of training images\n",
        "dataiter = iter(train_loader) #now contains the first batch\n",
        "images, labels = dataiter.next() #images=the first batch of images, labels= the first batch of labels\n",
        "images = images.numpy() #convert the images to display them\n",
        "\n",
        "#plot the imahes in the batch along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25,6))\n",
        "\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(1, 20, idx+1, xticks=[], yticks=[]) #(rows, cols, index, .., ..)\n",
        "  imshow(images[idx])\n",
        "  ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x-NXUdVCrosR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load AlexNet pretrained model\n",
        "model = models.vgg16(pretrained=True)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kQKIsrcLrosV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#freeze the model calssifier\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(25088, 12544)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dropout', nn.Dropout(0.5)),\n",
        "                          ('fc2', nn.Linear(12544, 10))]))\n",
        "#('output', nn.LogSoftmax(dim=1)\n",
        "\n",
        "model.classifier = classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZrb7HFbrosZ",
        "colab_type": "text"
      },
      "source": [
        "<h2>Choose how to train</h2>\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "    <ul>\n",
        "        <li>\n",
        "            <a href=\"#normal\">Normal Training</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#cyclical\">With Cyclical Learning Rate</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#inference\">Test our model</a>\n",
        "        </li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhyOqjherosb",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"normal\"></a>\n",
        "## Normal Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_5vU1BfCrosc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "#Loss function and optmixation function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GCP2OQcFrosi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "import numpy as np\n",
        "n_epochs = 60\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss (comapre the values of the output model to the actual labels)\n",
        "        loss = criterion(output, labels)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for images, labels in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, labels)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*images.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        valid_loss_min = valid_loss\n",
        "        # print the decremnet in the validation\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, train_loss, valid_loss))\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min, \n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_AML_classifier.pt')\n",
        "        \n",
        "    if epoch % 10 == 0:    \n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, train_loss, valid_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "W1df4w5hrosn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('model_AML_classifier.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rQ5l4d0crosq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize the test loss\n",
        "test_loss = 0.0\n",
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list (0. for i in range(10))\n",
        "\n",
        "#set the model to test and validation mode (no gradient descent needed)\n",
        "model.eval()\n",
        "\n",
        "for data, target in test_loader:\n",
        "  #move the tensor to GPU ig available\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  #forward pass: compute prediction output by passing the first batch of test data\n",
        "  output = model(data)\n",
        "  #calculate the batch size\n",
        "  loss = criterion(output, target)\n",
        "  #update the test loss\n",
        "  test_loss += loss.item()*data.size(0)\n",
        "  #convert output probabilities to output class\n",
        "  _, pred = torch.max(output, 1)\n",
        "  #compare the prediction to true label\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  #conveert to numpy array and remove the extra dimention and get only the result\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "  #calculate test accuracy for each object class\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      label = target.data[i] #get the corresponding label from the object\n",
        "      class_correct[label] += correct[i].item()\n",
        "      class_total[label] += 1\n",
        "    except IndexError:\n",
        "      break\n",
        "  \n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "  if class_total[i] > 0:\n",
        "     print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "  else:\n",
        "       print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NlLnCKbjrosv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Move model inputs to cuda\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "#get sample outputs\n",
        "output = model(images)\n",
        "#convert probabilties to prediction class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(10):\n",
        "    ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images.cpu()[idx])\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfdxwX8Fros1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"cyclical\"></a>\n",
        "## Implementing cyclical learning rate techneque:\n",
        "\n",
        "- First we need to find the max_lr and the base_lr.\n",
        "- implement the following experience to do so\n",
        "Refernce:\n",
        "- https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n",
        "- https://arxiv.org/pdf/1506.01186.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mDsy3228ros3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment parameters\n",
        "lr_find_epochs = 2\n",
        "start_lr = 1e-7\n",
        "end_lr = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mI5oOoRFros6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the model, optimizer and loss function for the experiment\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=start_lr)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# y = a.e(-bt) \n",
        "# end_lr = start_lr . e(b.t)\n",
        "# (end_lr - start_lr) = e(b.t)\n",
        "# ln(end_lr - start_lr) = b.t\n",
        "# b = ln(end_lr - start_lr) / t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "G_fRfW9Oros_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LR function lambda\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( train_loader)))\n",
        "scheduler = LambdaLR(optimizer, lr_lambda)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPSq4XjIrotE",
        "colab_type": "text"
      },
      "source": [
        "- In the following we run two epochs through the network. At each step we are capturing the LR and optimizing the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g1RQzrlcrotF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the experiment\n",
        "lr_find_loss = []\n",
        "lr_find_lr = []\n",
        "\n",
        "iter = 0\n",
        "\n",
        "smoothing = 0.05\n",
        "for i in range(lr_find_epochs):\n",
        "  print(\"epoch {}\".format(i))\n",
        "  model.train()\n",
        "  for inputs, labels in train_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get outputs to calc loss\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update LR\n",
        "    scheduler.step()\n",
        "    lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "    lr_find_lr.append(lr_step)\n",
        "\n",
        "    # smooth the loss\n",
        "    if iter==0:\n",
        "      lr_find_loss.append(loss)\n",
        "    else:\n",
        "      loss = smoothing  * loss + (1 - smoothing) * lr_find_loss[-1]\n",
        "      lr_find_loss.append(loss)\n",
        "     \n",
        "    iter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q74w57i1rotK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move model to GPU\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0G-063KwrotR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"learning rate\")\n",
        "plt.xscale(\"log\")\n",
        "plt.plot(lr_find_lr, lr_find_loss)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHLG2vSMrotW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- From the figure above, For the upper bound (max). We won't pick the one on lowest point but rather about a factor of ten to the left (0.5 is the lowest point). \n",
        "In this case\n",
        "the lowest point is about 3e-2 so we'll take 3e-2\n",
        "- Now for the lowe bound (Min): Acording to the paper and other resouce a good lower bound is the upper divided by 6. So 3e-3/6 = 5e-4\n",
        "\n",
        "- This approach could also help us find the range of acceptable lr for our model even if we decided to go with a fixed lr. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Z3ciTujArotY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As concluded above\n",
        "lr_max = 3e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NuA0efurotc",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: The CLR Scheduale\n",
        "Which varies the learning rate between uper and lower bound.\n",
        "we are going with triangular CLR schedule.\n",
        "![triangular schedule](https://drive.google.com/file/d/1K6GraTrK4oV7GOokhFewG-xOmHqKbj75/view?usp=sharing)\n",
        "- Programatically we just need to create a custom function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "T5aEB_Dbrotd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n",
        "\n",
        "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
        "    scaler = lambda x: 1.\n",
        "\n",
        "    # Lambda function to calculate the LR\n",
        "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
        "\n",
        "    # Additional function to see where on the cycle we are\n",
        "    def relative(it, stepsize):\n",
        "        cycle = math.floor(1 + it / (2 * stepsize))\n",
        "        x = abs(it / stepsize - 2 * cycle + 1)\n",
        "        return max(0, (1 - x)) * scaler(cycle)\n",
        "\n",
        "    return lr_lambda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pe5Gtdzrotg",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Wrap it up\n",
        "- This can be wrapped up inside LamdaLR object in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-TdmnVlYrotg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "#Parameters\n",
        "factor = 6\n",
        "end_lr = lr_max\n",
        "iter = 0\n",
        "total_logs = []\n",
        "\n",
        "#Loss function and optmixation function\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1.)\n",
        "step_size = 4*len(train_loader)\n",
        "clr = cyclical_lr(step_size, min_lr=end_lr/factor, max_lr=end_lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])\n",
        "\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIZrs9K9rotj",
        "colab_type": "text"
      },
      "source": [
        "# Step 4: Time to train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TqU0stHArotk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "import numpy as np\n",
        "n_epochs = 30\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss (comapre the values of the output model to the actual labels)\n",
        "        loss = criterion(output, labels)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step() # > Where the magic happens\n",
        "        lr_sched_test = scheduler.get_last_lr()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    \n",
        "    for images, labels in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, labels)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*images.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        # print the decremnet in the validation\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tLearning rate: {}'.format(\n",
        "            epoch, train_loss, valid_loss, lr_sched_test))\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min, \n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_AML_classifier.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "    if epoch % 10 == 0:    \n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tLearning rate: {}'.format(\n",
        "            epoch, train_loss, valid_loss, lr_sched_test))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEl5vwFMrott",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"inference\"></a>\n",
        "## Test our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mwxd6vONrott",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize the test loss\n",
        "test_loss = 0.0\n",
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list (0. for i in range(10))\n",
        "\n",
        "#set the model to test and validation mode (no gradient descent needed)\n",
        "model.eval()\n",
        "\n",
        "for data, target in test_loader:\n",
        "  #move the tensor to GPU ig available\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  #forward pass: compute prediction output by passing the first batch of test data\n",
        "  output = model(data)\n",
        "  #calculate the batch size\n",
        "  loss = criterion(output, target)\n",
        "  #update the test loss\n",
        "  test_loss += loss.item()*data.size(0)\n",
        "  #convert output probabilities to output class\n",
        "  _, pred = torch.max(output, 1)\n",
        "  #compare the prediction to true label\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  #conveert to numpy array and remove the extra dimention and get only the result\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "  #calculate test accuracy for each object class\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      label = target.data[i] #get the corresponding label from the object\n",
        "      class_correct[label] += correct[i].item()\n",
        "      class_total[label] += 1\n",
        "    except IndexError:\n",
        "      break\n",
        "  \n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "  if class_total[i] > 0:\n",
        "     print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "  else:\n",
        "       print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qRc2CcOhrotw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the experiment\n",
        "lr_find_loss = []\n",
        "lr_find_lr = []\n",
        "\n",
        "iter = 0\n",
        "\n",
        "smoothing = 0.05\n",
        "for i in range(lr_find_epochs):\n",
        "  print(\"epoch {}\".format(i))\n",
        "  model.train()\n",
        "  for inputs, labels in train_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get outputs to calc loss\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update LR\n",
        "    scheduler.step()\n",
        "    lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "    lr_find_lr.append(lr_step)\n",
        "\n",
        "    # smooth the loss\n",
        "    if iter==0:\n",
        "      lr_find_loss.append(loss)\n",
        "    else:\n",
        "      loss = smoothing  * loss + (1 - smoothing) * lr_find_loss[-1]\n",
        "      lr_find_loss.append(loss)\n",
        "     \n",
        "    iter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "OR8FUG14rotz",
        "colab_type": "text"
      },
      "source": [
        "### Notes:\n",
        "\n",
        "- make sure that the tarin, test, validation sets are randomly picked so they containe images from all calsses.\n",
        "- less lr better (stable) training\n",
        "- make sure my data is well distrbuted (more images most be obtaine)\n",
        "- The deeper the CNN the less FC layers is needed. because it needs less work to extract details and patterns since most of the job has been don by the CNN.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jt-A0iRorotz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}